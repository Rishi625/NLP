{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dasmiq/cs6120-assignment4/blob/main/decoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu66L-I5tsXo"
      },
      "source": [
        "# Language Model Decoding Algorithms\n",
        "\n",
        "In this notebook, you will implement several language model decoding algorithms, from simple greedy decoding to beam search. You will test these algorithms using GPT-2 as implemented in the huggingface `transformers` library. You will evaluate the results of an open-ended story generation task, where the goal is not to match a particular human-written reference text and more to generate text that is fluent and diverse. This gives more scope to different decoding approaches.\n",
        "\n",
        "The `transformers` library implements most of these decoding algorithms, but you will be implementing them yourself using the capabilities of the underlying pytorch library.\n",
        "\n",
        "This code will benefit from running on a GPU.  If you're running on Colab, **choose \"Runtime Type\" = GPU for running this notebook (Runtime > Change Runtime Type > T4 GPU).**\n",
        "\n",
        "The evaluation and harness code for beam search is based on an earlier assignment by Jaehun Jung, Gary Jiacheng, and Yejin Choi from the University of Washington.\n",
        "\n",
        "## Setup\n",
        "\n",
        "Please make sure to run all cells in the setup section, even though you won't implement anything yet. First, we install the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1lSg7SGaR5fM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
            "Requirement already satisfied: filelock in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2.3.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2025.9.18)\n",
            "Requirement already satisfied: requests in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (2025.8.3)\n",
            "Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.57.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: datasets in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (4.2.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (2.3.3)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (21.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (0.4.0)\n",
            "Requirement already satisfied: pandas in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (2.3.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (2.32.5)\n",
            "Requirement already satisfied: httpx<1.0.0 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (0.35.3)\n",
            "Requirement already satisfied: packaging in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: anyio in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: evaluate in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (0.4.6)\n",
            "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from evaluate) (4.2.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from evaluate) (2.3.3)\n",
            "Requirement already satisfied: dill in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from evaluate) (0.4.0)\n",
            "Requirement already satisfied: pandas in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from evaluate) (2.3.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from evaluate) (2.32.5)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from evaluate) (0.35.3)\n",
            "Requirement already satisfied: packaging in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from datasets>=2.0.0->evaluate) (21.0.0)\n",
            "Requirement already satisfied: httpx<1.0.0 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from datasets>=2.0.0->evaluate) (0.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.19.0->evaluate) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.19.0->evaluate) (2025.8.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: anyio in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\rushi\\appdata\\roaming\\python\\python313\\site-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2HajK5vaM9N"
      },
      "source": [
        "Now we check available devices and set the randomizer seed for replicability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "H5S6UO84y3fj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cpu\n"
          ]
        }
      ],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'device: {device}')\n",
        "\n",
        "def set_seed(seed=19260817):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S8uX6e1uCEU"
      },
      "source": [
        "We load a dataset of prompts for open-ended story generation derived from the [ROCStories corpora](https://cs.rochester.edu/nlp/rocstories/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kPcsD3_Gqm7P"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\rushi\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'story_id': '080198fc-d0e7-42b3-8e63-b2144e59d816', 'prompt': 'On my way to work I stopped to get some coffee.', 'continuation': 'I went through the drive through and placed my order. I paid the cashier and patiently waited for my drink. When she handed me the drink, the lid came off and spilled on me. The coffee hurt and I had to go home and change clothes.', 'constraint_words': ['drive', 'order', 'drink', 'lid', 'coffee', 'hurt', 'home', 'change', 'clothes']}\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('Ximing/ROCStories')\n",
        "train_data, dev_data, test_data = dataset['train'], dataset['validation'], dataset['test']\n",
        "\n",
        "print(train_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_dqyjUBuQlQ"
      },
      "source": [
        "Unlike more constrained generation tasks such as translation and summarization, we cannot usefully check the generated output against a single human reference. Instead, we evaluate its overall fluency, diversity, and similarity to other English texts.\n",
        "\n",
        "To score the English **fluency** of generated sentences, we will use the CoLA classifier.  This is a RoBERTa-large classifier trained on the CoLA corpus (Warstadt et al., 2019), which contains sentences paired with grammatical acceptability judgments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gmMd57VjSEZY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at textattack/roberta-base-CoLA were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from evaluate import load\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
        "\n",
        "perplexity_scorer = load(\"perplexity\", module_type=\"metric\")\n",
        "cola_model_name = \"textattack/roberta-base-CoLA\"\n",
        "cola_tokenizer = RobertaTokenizer.from_pretrained(cola_model_name)\n",
        "cola_model = RobertaForSequenceClassification.from_pretrained(cola_model_name).to(device)\n",
        "\n",
        "def batchify(data, batch_size):\n",
        "    assert batch_size > 0\n",
        "\n",
        "    batch = []\n",
        "    for item in data:\n",
        "        # Yield next batch\n",
        "        if len(batch) == batch_size:\n",
        "            yield batch\n",
        "            batch = []\n",
        "\n",
        "        batch.append(item)\n",
        "\n",
        "    # Yield last un-filled batch\n",
        "    if len(batch) != 0:\n",
        "        yield batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-m8_EsPGFQU"
      },
      "source": [
        "To evaluate **diversity**, we will simply count the number of distinct 1-, 2-, and 3-grams in the output.\n",
        "\n",
        "To evaluate **naturalness**, or similarity to other English texts, we will use the perplexity of the output under the model, i.e., GPT-2. Think about which decoding method you would expect to perform the best on this metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7t97XqNh2Xu2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "debugging run\n",
            "perplexity = 178.62\n",
            "fluency = 0.98\n",
            "diversity = 0.87, 1.00, 1.00\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def compute_perplexity(texts, model='gpt2', batch_size=8):\n",
        "    score = perplexity_scorer.compute(predictions=texts, add_start_token=True, batch_size=batch_size, model_id=model)\n",
        "    return score['mean_perplexity']\n",
        "\n",
        "\n",
        "def compute_fluency(texts, batch_size=8):\n",
        "  scores = []\n",
        "  for b_texts in batchify(texts, batch_size):\n",
        "    inputs = cola_tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "      logits = cola_model(**inputs).logits\n",
        "      probs = logits.softmax(dim=-1)\n",
        "      scores.extend(probs[:, 1].tolist())\n",
        "  return sum(scores) / len(scores)\n",
        "\n",
        "\n",
        "def compute_diversity(texts):\n",
        "    unigrams, bigrams, trigrams = [], [], []\n",
        "    total_words = 0\n",
        "    for gen in texts:\n",
        "        o = gen.split(' ')\n",
        "        total_words += len(o)\n",
        "        for i in range(len(o)):\n",
        "            unigrams.append(o[i])\n",
        "        for i in range(len(o) - 1):\n",
        "            bigrams.append(o[i] + '_' + o[i + 1])\n",
        "        for i in range(len(o) - 2):\n",
        "            trigrams.append(o[i] + '_' + o[i + 1] + '_' + o[i + 2])\n",
        "    return len(set(unigrams)) / len(unigrams), len(set(bigrams)) / len(bigrams), len(set(trigrams)) / len(trigrams)\n",
        "\n",
        "\n",
        "def evaluate(generations, experiment):\n",
        "  generations = [_ for _ in generations if _ != '']\n",
        "  perplexity = compute_perplexity(generations)\n",
        "  fluency = compute_fluency(generations)\n",
        "  diversity = compute_diversity(generations)\n",
        "  print(experiment)\n",
        "  print(f'perplexity = {perplexity:.2f}')\n",
        "  print(f'fluency = {fluency:.2f}')\n",
        "  print(f'diversity = {diversity[0]:.2f}, {diversity[1]:.2f}, {diversity[2]:.2f}')\n",
        "  print()\n",
        "\n",
        "debug_sents = [\"This restaurant is awesome\", \"My dog is cute and I love it.\", \"Today is sunny.\"]\n",
        "evaluate(debug_sents, 'debugging run')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGaWrCTzzIqA"
      },
      "source": [
        "We install GPT-2 and its associated tokenizer from huggingface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn5_8Bym3HAq",
        "outputId": "93f94eaf-d467-4ab1-c49a-20de68cb1b47"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name, pad_token=\"<|endoftext|>\")\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqgPTfEYHJso"
      },
      "source": [
        "## Batching and Working with Tensors\n",
        "\n",
        "To improve the throughput of language model training and inference, especially on GPUs, we usually group together inputs in **batches**.  What this means for your generation code is that at each function call, you will decide on the next token for several inputs in parallel. Instead of a single vector of the pre-softmax logits of the next token, you will get a two-dimensional tensor of shape $B \\times V$ for batch size $B$ and token vocabulary size $V$.\n",
        "\n",
        "Consider a toy example of a random $4 \\times 5$ matrix $A$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BgsquTH9na9S"
      },
      "outputs": [],
      "source": [
        "A = torch.randn(4, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26VCjPMSnrDM"
      },
      "source": [
        "**TODO**: Familiarize yourself with some of the functions on tensors that will be useful for implementing the different decoding algorithms below: `torch.argmax`, `torch.multinomial`, `torch.nn.functional.softmax`, `torch.squeeze`, `torch.topk` and others. You can consult [pytorch's documentation](https://docs.pytorch.org/docs/stable/index.html) or other resources. These functions and others in the pytorch library are all fine to use in your code below. This cell is not graded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FlU641QspQxE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original matrix A:\n",
            "tensor([[ 0.6351, -0.6377, -0.1440, -0.3880, -0.3604],\n",
            "        [ 0.2237,  1.5148,  0.8032,  0.4162, -1.7408],\n",
            "        [ 0.0202,  0.5593,  1.3399,  0.5249, -0.4227],\n",
            "        [ 0.7167,  0.0528, -0.3531,  0.9206,  0.3722]])\n",
            "\n",
            "Argmax along dim=1 (max in each row): tensor([0, 1, 2, 3])\n",
            "Argmax along dim=0 (max in each column): tensor([3, 1, 2, 3, 3])\n",
            "\n",
            "Softmax along dim=1:\n",
            "tensor([[0.4052, 0.1135, 0.1859, 0.1457, 0.1497],\n",
            "        [0.1286, 0.4678, 0.2296, 0.1559, 0.0180],\n",
            "        [0.1142, 0.1958, 0.4274, 0.1892, 0.0733],\n",
            "        [0.2637, 0.1358, 0.0905, 0.3233, 0.1868]])\n",
            "Row sums after softmax (should be 1): tensor([1.0000, 1.0000, 1.0000, 1.0000])\n",
            "\n",
            "Top 2 values in each row: tensor([[ 0.6351, -0.1440],\n",
            "        [ 1.5148,  0.8032],\n",
            "        [ 1.3399,  0.5593],\n",
            "        [ 0.9206,  0.7167]])\n",
            "Their indices: tensor([[0, 2],\n",
            "        [1, 2],\n",
            "        [2, 1],\n",
            "        [3, 0]])\n",
            "\n",
            "Sampled indices from each row: tensor([4, 1, 1, 2])\n",
            "\n",
            "Shape after unsqueeze: torch.Size([1, 4, 5])\n",
            "Shape after squeeze: torch.Size([4, 5])\n"
          ]
        }
      ],
      "source": [
        "# TODO: Familiarize yourself with some pytorch functions by manipulating the matrix A.\n",
        "\n",
        "# Testing torch.argmax - finds index of maximum value\n",
        "print(\"Original matrix A:\")\n",
        "print(A)\n",
        "print(\"\\nArgmax along dim=1 (max in each row):\", torch.argmax(A, dim=1))\n",
        "print(\"Argmax along dim=0 (max in each column):\", torch.argmax(A, dim=0))\n",
        "\n",
        "# Testing torch.nn.functional.softmax - converts to probabilities\n",
        "print(\"\\nSoftmax along dim=1:\")\n",
        "print(F.softmax(A, dim=1))\n",
        "print(\"Row sums after softmax (should be 1):\", F.softmax(A, dim=1).sum(dim=1))\n",
        "\n",
        "# Testing torch.topk - get top k values and indices\n",
        "top2_values, top2_indices = torch.topk(A, k=2, dim=1)\n",
        "print(\"\\nTop 2 values in each row:\", top2_values)\n",
        "print(\"Their indices:\", top2_indices)\n",
        "\n",
        "# Testing torch.multinomial - sample from probability distribution\n",
        "probs = F.softmax(A, dim=1)\n",
        "samples = torch.multinomial(probs, num_samples=1)\n",
        "print(\"\\nSampled indices from each row:\", samples.squeeze())\n",
        "\n",
        "# Testing torch.squeeze - remove dimensions of size 1\n",
        "B = A.unsqueeze(0)  # Add dimension: (1, 4, 5)\n",
        "print(\"\\nShape after unsqueeze:\", B.shape)\n",
        "print(\"Shape after squeeze:\", B.squeeze().shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfwAdXU4urVp"
      },
      "source": [
        "## Basic Decoding Algorithms\n",
        "\n",
        "In this section, you will implement a few basic decoding algorithms:\n",
        "1. Greedy decoding\n",
        "2. Simple sampling\n",
        "3. Temperature sampling\n",
        "4. Top-k sampling\n",
        "5. Top-p sampling\n",
        "\n",
        "We have provided a wrapper function `decode()` that takes care of batching, controlling max length, and handling the EOS token.\n",
        "You will be asked to implement the core function of each method: *given the pre-softmax logits of the next token, decide what the next token is.*\n",
        "\n",
        "**The wrapper calls the core function of each decoding algorithm, which you will implement in the subsections below.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kSHpk5vKyQ4U"
      },
      "outputs": [],
      "source": [
        "def decode(prompts, max_len, method, **kwargs):\n",
        "  encodings_dict = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
        "  input_ids = encodings_dict['input_ids'].to(device)\n",
        "  attention_mask = encodings_dict['attention_mask'].to(device)\n",
        "\n",
        "  model_kwargs = {\n",
        "      'attention_mask': attention_mask,\n",
        "      'cache_position': torch.arange(input_ids.shape[1], device=device)\n",
        "  }\n",
        "  batch_size, input_seq_len = input_ids.shape\n",
        "\n",
        "  unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=device)\n",
        "\n",
        "  for step in range(max_len):\n",
        "    model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
        "    with torch.no_grad():\n",
        "      outputs = model(**model_inputs, return_dict=True, output_attentions=False, output_hidden_states=False)\n",
        "\n",
        "    if step == 0:\n",
        "      last_non_masked_idx = torch.sum(attention_mask, dim=1) - 1\n",
        "      next_token_logits = outputs.logits[range(batch_size), last_non_masked_idx, :]\n",
        "    else:\n",
        "      next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "    log_prob = F.log_softmax(next_token_logits, dim=-1)\n",
        "\n",
        "    if method == 'greedy':\n",
        "      next_tokens = greedy(next_token_logits)\n",
        "    elif method == 'sample':\n",
        "      next_tokens = sample(next_token_logits)\n",
        "    elif method == 'temperature':\n",
        "      next_tokens = temperature(next_token_logits, t=kwargs.get('t', 0.8))\n",
        "    elif method == 'topk':\n",
        "      next_tokens = topk(next_token_logits, k=kwargs.get('k', 20))\n",
        "    elif method == 'topp':\n",
        "      next_tokens = topp(next_token_logits, p=kwargs.get('p', 0.7))\n",
        "\n",
        "    # finished sentences should have their next token be a padding token\n",
        "    next_tokens = next_tokens * unfinished_sequences + tokenizer.pad_token_id * (1 - unfinished_sequences)\n",
        "\n",
        "    input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
        "    model_kwargs = model._update_model_kwargs_for_generation(outputs, model_kwargs, is_encoder_decoder=model.config.is_encoder_decoder)\n",
        "\n",
        "    # if eos_token was found in one sentence, set sentence to finished\n",
        "    unfinished_sequences = unfinished_sequences.mul((next_tokens != tokenizer.eos_token_id).long())\n",
        "\n",
        "    if unfinished_sequences.max() == 0:\n",
        "      break\n",
        "\n",
        "  response_ids = input_ids[:, input_seq_len:]\n",
        "  response_text = [tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True) for output in response_ids]\n",
        "\n",
        "  return response_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "P-bghfZe30wN"
      },
      "outputs": [],
      "source": [
        "# For debugging, we duplicate a single prompt 10 times so that we obtain 10 generations for the same prompt\n",
        "dev_prompts = [dev_data[0]['prompt']] * 10\n",
        "\n",
        "def print_generations(prompts, generations):\n",
        "  for prompt, generation in zip(prompts, generations):\n",
        "    print(f'{[prompt]} ==> {[generation]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ryFGrlRSXYn"
      },
      "source": [
        "### Greedy Decoding\n",
        "\n",
        "The simplest strategy is to select the token with the highest probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xH0wBhy2SZa-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n"
          ]
        }
      ],
      "source": [
        "# TODO: Implement greedy decoding\n",
        "\n",
        "def greedy(next_token_logits):\n",
        "  '''\n",
        "  inputs:\n",
        "  - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "  outputs:\n",
        "  - next_tokens: Tensor(size = (B), dtype = long)\n",
        "  '''\n",
        "\n",
        "  # Compute `next_tokens` from `next_token_logits`.\n",
        "  next_tokens = torch.argmax(next_token_logits, dim=-1)\n",
        "\n",
        "  return next_tokens\n",
        "\n",
        "\n",
        "generations = decode(dev_prompts, max_len=20, method='greedy')\n",
        "print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYghJYko1X13"
      },
      "source": [
        "**TODO**: What do you observe when the code above generates 10 times from the test prompt?\n",
        "\n",
        "Answer : All 10 generations produce exactly the same output. Despite running the generation 10 times on the same prompt \"Ryan was called by his friend to skip work one day.\", every single output is identical: '\\n\\n\"I was like, 'I'm going to go to work tomorrow,'\" he said.' This demonstrates that greedy decoding is completely deterministic - since it always selects the token with the highest probability at each step, there is no variation in the output. This lack of diversity is a key limitation of greedy decoding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGKL_31VJNw1"
      },
      "source": [
        "### Simple Sampling and Temperature Sampling\n",
        "\n",
        "Greedy decoding has some drawbacks that we discussed in class. One alternative is simply to sample from the probability distribution of the language model. We'll call that _simple sampling_ to distinguish it from more complicated methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PjrTLKj2JR5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ryan was called by his friend to skip work one day.'] ==> [' During the part of the interview, he commented that he had nosy friends in the general population,']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [\" So he realized later that things couldn't get better. He was 27 and made his move to California\"]\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\nHe told Armin, who recorded an apology to his teacher, \"It\\'s unbear']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' Allow me to pose sadly for yours, Open Your Hearts and Heartbroken Princess and what?? Slovakia 5']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' \"(Chavez)\" was heard saying, \"Chyromosomes.\"\\n\\nImages from Amin']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' He rarely went to the mall. Herein lies the problem: those minutes they spent playing baseball or']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' She also managed to get work as well. http://www.washingtonpost.com/news']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' He refused the cut and hung up.\"\\n\\nI remember Liderkov asking David Siller who']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [\" Her husband's teacher at them told her he'd stopped acting so less gratuitously.\\n\\n\"]\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' Unable to free himself and get home, Clavier made a payment. (credit report via Yahooca']\n"
          ]
        }
      ],
      "source": [
        "# TODO: Implement simple sampling.\n",
        "def sample(next_token_logits):\n",
        "  '''\n",
        "  inputs:\n",
        "  - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "  outputs:\n",
        "  - next_tokens: Tensor(size = (B), dtype = long)\n",
        "  '''\n",
        "\n",
        "  # Compute the probabilities from the logits,\n",
        "  # then compute `next_tokens` from `probs`.\n",
        "  # Hint: `probs` should have size (B, V)\n",
        "  probs = F.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "  next_tokens = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "  return next_tokens\n",
        "\n",
        "\n",
        "set_seed()\n",
        "generations = decode(dev_prompts, max_len=20, method='sample')\n",
        "print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fU-jtevc1_yO"
      },
      "source": [
        "**TODO**: What do you observe when you generating 10 times from the test prompt?\n",
        "\n",
        "Answer: All 10 generations are completely different from each other, despite starting from the same prompt \"Ryan was called by his friend to skip work one day.\" The outputs vary wildly in content and style - some are coherent continuations about work/friends, while others diverge into unrelated topics like \"Slovakia 5\", \"Chyromosomes\", URLs, and even nonsensical phrases. This demonstrates that simple sampling introduces significant randomness by sampling from the entire probability distribution, including low-probability tokens. While this provides diversity (solving greedy's repetition problem), it can lead to incoherent or off-topic generations since even unlikely tokens have a chance of being selected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNFfm1kXp_4f"
      },
      "source": [
        "The probability distribution of the model has some undesirable properties, as we discussed in class. You can adjust its entropy using a _temperature_ parameter `t`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "25Su03uzJb_Z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I had a call from my friend, the nosy neighbor, saying, \\'I']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' So he went to the hotel where the rest of the family was staying and made his way to his']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"He was asked, \\'why aren\\'t you going to come to work?\\' \"Wilson']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' Allow me to introduce myself to yours truly, and explain how you joined us and what you had in']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' The police, however, looked him up and noticed that he was really funny, and quickly rounded on']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' He rarely went to the bathroom. Here are some more stories from those same stories.\\n\\nSo']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' She also managed to get him to go to the bank and pay for a car. He promised he']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' He refused the cut and made the cut again.\\n\\n\"I told him I never went to']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [\" Her husband's job at them was a sales associate. She was less than thrilled to leave his job\"]\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' Unable to find any work during his shift, he made a payment. (He may have been able']\n"
          ]
        }
      ],
      "source": [
        "# TODO: Implement temperature-scaled sampling.\n",
        "\n",
        "def temperature(next_token_logits, t):\n",
        "  '''\n",
        "  inputs:\n",
        "  - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "  - t: float\n",
        "  outputs:\n",
        "  - next_tokens: Tensor(size = (B), dtype = long)\n",
        "  '''\n",
        "\n",
        "  next_tokens = torch.multinomial(F.softmax(next_token_logits / t, dim=-1), num_samples=1).squeeze(-1)\n",
        "\n",
        "  return next_tokens\n",
        "\n",
        "\n",
        "set_seed()\n",
        "generations = decode(dev_prompts, max_len=20, method='temperature', t=0.8)\n",
        "print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDYIAzmF2MD2"
      },
      "source": [
        "**TODO**: What value of `t` would be equivalent to greedy decoding? What value of `t` would be equivalent to simple sampling?\n",
        "\n",
        "Answer: \n",
        "- t → 0 (approaching zero, like t = 0.001 or smaller) would be equivalent to greedy decoding. As temperature approaches 0, the softmax distribution becomes extremely peaked, with the highest logit dominating all others, effectively making the model always choose the most probable token.\n",
        "\n",
        "- t = 1.0 would be equivalent to simple sampling. When temperature equals 1, we have `logits / 1 = logits`, meaning no scaling is applied, and we're just doing regular softmax and sampling from the original distribution.\n",
        "\n",
        "- t < 1 (like 0.5, 0.8): Makes the model more conservative, favoring higher probability tokens\n",
        "- t > 1 (like 1.5, 2.0): Makes the model more creative/random, flattening the distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZZorDeWRVtm"
      },
      "source": [
        "### Top-k Sampling\n",
        "\n",
        "By Zipf's law, the distribution over tokens contains many low-probability words in the tail. It might be useful to sample only from the most probable words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hNHk9mjXRdNA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ryan was called by his friend to skip work one day.'] ==> [\" I'm sure it was a call he made because he had no idea he was working at his job\"]\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' \"She had to come home early because she was working in the store and her husband was late,\"']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\nHe told ABC News, \"I\\'ve got to do the next one.\"\\n\\nAccording']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' The day before, at 8:30 p.m., his friend told him what to do and']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' The police, however, were not allowed to see him.\\n\\n\"If they would have seen']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' He was told that he will be taken to police headquarters at 8:30 p.m. because']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' She got up and said, \"I got you out of class.\" He replied that if he didn']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' He refused the chance and then, in his attempt to get to work, decided to get a bike']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I saw them coming out with two people, so I was like \\'what\\'s wrong']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' \"I thought they were really stupid, but then I got home. I was so tired from the']\n"
          ]
        }
      ],
      "source": [
        "# TODO: Implement top-k sampling\n",
        "\n",
        "def topk(next_token_logits, k):\n",
        "  '''\n",
        "  inputs:\n",
        "  - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "  - k: int\n",
        "  outputs:\n",
        "  - next_tokens: Tensor(size = (B), dtype = long)\n",
        "  '''\n",
        "\n",
        "  # Keep only top-k tokens with highest probabilities.\n",
        "  # Hint: Create a mask to zero out all logits not in top-k\n",
        "\n",
        "  # Get top-k values and indices\n",
        "  top_k_values, top_k_indices = torch.topk(next_token_logits, k, dim=-1)\n",
        "  \n",
        "  # Create a mask with -inf for all positions\n",
        "  masked_logits = torch.full_like(next_token_logits, float('-inf'))\n",
        "  \n",
        "  # Put the top-k values back in their positions\n",
        "  masked_logits.scatter_(dim=-1, index=top_k_indices, src=top_k_values)\n",
        "\n",
        "  next_tokens = torch.multinomial(F.softmax(masked_logits, dim=-1), num_samples=1).squeeze(-1)\n",
        "\n",
        "  return next_tokens\n",
        "\n",
        "\n",
        "set_seed()\n",
        "generations = decode(dev_prompts, max_len=20, method='topk', k=20)\n",
        "print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHFioE3V3WAD"
      },
      "source": [
        "**TODO**: What value of `k` makes this equivalent to simple sampling?\n",
        "\n",
        "Answer: k = 50257 (the vocabulary size of GPT-2) would make top-k sampling equivalent to simple sampling. When k equals the total vocabulary size, we're keeping all possible tokens, so no filtering occurs and we sample from the complete probability distribution, just like simple sampling does."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSjMWNFEy_cC"
      },
      "source": [
        "### Top-p Sampling\n",
        "\n",
        "But should we sample from the same number of candidates in all contexts? Maybe in different contexts the number of good candidates is larger or smaller."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Oq1ZwVVxzApa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ryan was called by his friend to skip work one day.'] ==> [' I just turned it over to him and I could see the mental damage he was doing to his body']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' So he went to the hotel where the rest of the family was staying and made his way over to']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\nHe told ABC News, \"I\\'ve got to do this. I\\'ll miss the time']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' The day before, at 8:30 p.m., his grandmother told him to take his pants']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' The police, however, were not allowed to see him.\\n\\n\"I just came home and']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [\" He was told that he was fired because he hadn't done his job properly. He was dismissed because\"]\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' She also asked what was going on. \"I went and told him to go on a date and']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' He refused, saying he wanted to sleep on his own, not for a job.\\n\\nHe']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I am going to go out to the game,\" Bell said. \"That\\'s not']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' \"I thought it was really stupid, but then I got home,\" he says.\\n\\nBy']\n"
          ]
        }
      ],
      "source": [
        "# TODO: Implement top-p (nucleus) sampling.\n",
        "\n",
        "def topp(next_token_logits, p):\n",
        "  '''\n",
        "  inputs:\n",
        "  - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "  - p: float\n",
        "  outputs:\n",
        "  - next_tokens: Tensor(size = (B), dtype = long)\n",
        "  '''\n",
        "\n",
        "  # TODO: Sort the logits in descending order, and compute\n",
        "  # the cumulative probabilities `cum_probs` on the sorted logits\n",
        "  # and then sample from those that make up the top p probability mass.\n",
        "\n",
        "  # Sort logits in descending order\n",
        "  sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True, dim=-1)\n",
        "  \n",
        "  # Convert sorted logits to probabilities\n",
        "  sorted_probs = F.softmax(sorted_logits, dim=-1)\n",
        "  \n",
        "  # Compute cumulative probabilities\n",
        "  cum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "  \n",
        "  # Create mask for tokens to remove (those after cumulative prob exceeds p)\n",
        "  mask = cum_probs > p\n",
        "  # Shift mask to the right to include the token that crosses the threshold\n",
        "  mask[..., 1:] = mask[..., :-1].clone()\n",
        "  mask[..., 0] = False\n",
        "  \n",
        "  # Set logits of tokens to remove to -inf\n",
        "  sorted_logits[mask] = float('-inf')\n",
        "  \n",
        "  # Create tensor to store filtered logits in original order\n",
        "  filtered_logits = torch.full_like(next_token_logits, float('-inf'))\n",
        "  \n",
        "  # Scatter sorted logits back to their original positions\n",
        "  filtered_logits.scatter_(dim=-1, index=sorted_indices, src=sorted_logits)\n",
        "  \n",
        "  # Sample from the filtered distribution\n",
        "  next_tokens = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1).squeeze(-1)\n",
        "  \n",
        "  return next_tokens\n",
        "\n",
        "\n",
        "set_seed()\n",
        "generations = decode(dev_prompts, max_len=20, method='topp', p=0.7)\n",
        "print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB_lyjZr3i48"
      },
      "source": [
        "**TODO**: What value of `p` makes this equivalent to simple sampling?\n",
        "\n",
        "Answer: p = 1.0 makes top-p sampling equivalent to simple sampling. When p = 1.0, we include all tokens that contribute to 100% of the cumulative probability mass, which means we're keeping the entire vocabulary distribution, making it identical to simple sampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZElDTWEHCHNO"
      },
      "source": [
        "### Evaluate!\n",
        "\n",
        "Run the following cell to obtain the evaluation results, which you should include in your writeup.\n",
        "Also don't forget to answer the questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2UjZ-31s449u"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [01:41<00:00, 10.10s/it]\n",
            "100%|██████████| 13/13 [00:15<00:00,  1.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "greedy\n",
            "perplexity = 2.08\n",
            "fluency = 0.78\n",
            "diversity = 0.01, 0.02, 0.03\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [01:46<00:00, 10.60s/it]\n",
            "100%|██████████| 13/13 [00:17<00:00,  1.31s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sample\n",
            "perplexity = 61.67\n",
            "fluency = 0.37\n",
            "diversity = 0.42, 0.89, 0.99\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [01:44<00:00, 10.49s/it]\n",
            "100%|██████████| 13/13 [00:15<00:00,  1.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "temperature\n",
            "perplexity = 16.15\n",
            "fluency = 0.66\n",
            "diversity = 0.31, 0.77, 0.96\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [01:46<00:00, 10.69s/it]\n",
            "100%|██████████| 13/13 [00:15<00:00,  1.20s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "topk\n",
            "perplexity = 27.46\n",
            "fluency = 0.70\n",
            "diversity = 0.26, 0.74, 0.96\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [02:14<00:00, 13.49s/it]\n",
            "100%|██████████| 13/13 [00:16<00:00,  1.27s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "topp\n",
            "perplexity = 12.01\n",
            "fluency = 0.72\n",
            "diversity = 0.29, 0.76, 0.95\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompts = [item['prompt'] for item in test_data][:10]\n",
        "GENERATIONS_PER_PROMPT = 10\n",
        "MAX_LEN = 100\n",
        "\n",
        "for experiment in ['greedy', 'sample', 'temperature', 'topk', 'topp']:\n",
        "  generations = []\n",
        "  for prompt in tqdm(prompts):\n",
        "    generations += decode([prompt] * GENERATIONS_PER_PROMPT, max_len=MAX_LEN, method=experiment)\n",
        "  evaluate(generations, experiment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjlTLFH94Qlt"
      },
      "source": [
        "**TODO**: Which method has the best and worst perplexity? Fluency? Diversity?\n",
        "\n",
        "Answer: \n",
        "\n",
        "**Perplexity (lower is better):**\n",
        "- **Best**: Greedy (2.08) - follows the model's highest probability path, resulting in the most \"expected\" text according to the model\n",
        "- **Worst**: Sample (61.67) - unrestricted sampling includes many low-probability tokens, creating less predictable text\n",
        "\n",
        "**Fluency (higher is better):**\n",
        "- **Best**: Greedy (0.78) - consistently selecting high-probability tokens produces grammatically correct text\n",
        "- **Worst**: Sample (0.37) - sampling from the entire distribution often selects unlikely tokens that harm grammatical coherence\n",
        "\n",
        "**Diversity (higher is better):**\n",
        "- **Best**: Sample (0.42, 0.89, 0.99) - maximum randomness creates the most varied outputs\n",
        "- **Worst**: Greedy (0.01, 0.02, 0.03) - deterministic selection produces nearly identical outputs with minimal diversity\n",
        "\n",
        "**Key observation**: There's a clear trade-off between quality (perplexity/fluency) and diversity. Greedy excels at quality but lacks diversity, while sampling maximizes diversity at the cost of quality. The constrained sampling methods (temperature, top-k, top-p) offer middle ground, with top-p (p=0.7) achieving a particularly good balance (perplexity=12.01, fluency=0.72, decent diversity)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv0P_mt1RWh6"
      },
      "source": [
        "## Beam Search\n",
        "\n",
        "In this part of the assignment, your main task is to implement beam search algorithm. While debugging for implementation, we recommend setting `device` to `cpu`, as debugging would not require too much resource for this assignment. Once you are done with implementation, you may switch `device` to `cuda` and choose `Colab Runtime Type = GPU` for faster evaluation of your algorithm.\n",
        "\n",
        "We have provided some helpful functions and classes. Please make sure that you understand them. But you don't need to implement/change anything of them until the code block marked TODO."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPbvq4LnRd_j"
      },
      "source": [
        "### Configurations: load model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "78dmI0HW4P-h"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GenerationConfig\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name, pad_token=\"<|endoftext|>\")\n",
        "tokenizer.padding_side = \"left\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "model.eval()\n",
        "\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "eos_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq9lOjrwRSrs"
      },
      "source": [
        "### Helper classes: `BeamHypothesisList` and `BeamManager`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Ut9kXr0LRf4n"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class BeamHypothesis:\n",
        "    def __init__(self, input_ids: torch.LongTensor, score: torch.FloatTensor | float):\n",
        "        self.input_ids: torch.LongTensor = input_ids  # a single token sequence of size (seq_len,)\n",
        "        self.score: torch.FloatTensor = score  # a scalar score for the token sequence\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"BeamHypothesis(input_ids: {self.input_ids}, score: {self.score})\"\n",
        "\n",
        "class BeamHypothesisList:\n",
        "    def __init__(self, beam_width: int):\n",
        "        self.beam_hypotheses: List[BeamHypothesis] = []  # list of beam_hypothesis\n",
        "        self.beam_width: int = beam_width\n",
        "\n",
        "        self.worst_score = 1e9  # worst beam score in self.beam_hypotheses\n",
        "\n",
        "    def add(self, new_input_ids: torch.LongTensor, sum_logprobs: float):\n",
        "        \"\"\"\n",
        "        :param new_input_ids: new token sequence of size (1, seq_len)\n",
        "        :param sum_logprobs: sum of log probabilities of tokens in new_input_ids\n",
        "        Given a new hypothesis (new_input_ids) and its corresponding sum_logprobs, update self.beam_hypotheses with a finished hypothesis.\n",
        "        (1) If the new_input_ids has higher score than the current worst score in self.beam_hypotheses,\n",
        "            we replace the worst one with the new hypothesis.\n",
        "        (2) Otherwise, we do not make any change to self.beam_hypotheses.\n",
        "        \"\"\"\n",
        "        # For score, we compute average token log probability\n",
        "        score = sum_logprobs / new_input_ids.size(-1)\n",
        "\n",
        "        # add the new hypothesis if we still have vacant beams\n",
        "        if len(self.beam_hypotheses) < self.beam_width:\n",
        "            # Initialize the new_beam_hypothesis using new_input_ids and score\n",
        "            new_beam_hypothesis: BeamHypothesis = BeamHypothesis(input_ids=new_input_ids, score=score)\n",
        "\n",
        "            # Add new_beam_hypothesis to beam_hypotheses\n",
        "            self.beam_hypotheses.append(new_beam_hypothesis)\n",
        "\n",
        "        # even if the beam_hypotheses are full, if the new hypothesis has higher score than the worst hypothesis, we replace the worst hypothesis\n",
        "        elif score > self.worst_score:\n",
        "            # Remove the worst hypothesis, the one with the lowest score in self.beam_hypotheses\n",
        "            worst_hypothesis: BeamHypothesis = min(self.beam_hypotheses, key=lambda hyp: hyp.score)\n",
        "            self.beam_hypotheses.remove(worst_hypothesis)\n",
        "\n",
        "            # Add new hypothesis\n",
        "            # Initialize the new_beam_hypothesis using new_input_ids and score\n",
        "            new_beam_hypothesis = BeamHypothesis(input_ids=new_input_ids, score=score)\n",
        "\n",
        "            # Add new_beam_hypothesis to beam_hypotheses\n",
        "            self.beam_hypotheses.append(new_beam_hypothesis)\n",
        "\n",
        "        # Update the worst score - the lowest score among all beams in self.beam_hypotheses\n",
        "        self.worst_score = min(float(hyp.score) for hyp in self.beam_hypotheses)\n",
        "\n",
        "        # Sanity check\n",
        "        assert len(self.beam_hypotheses) <= self.beam_width"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2HD5CoYBSvpE"
      },
      "outputs": [],
      "source": [
        "class BeamManager:\n",
        "    def __init__(self, batch_size: int, beam_width: int):\n",
        "        self.finished_beam_hypotheses_list = [BeamHypothesisList(beam_width) for _ in range(batch_size)]\n",
        "        self.batch_size = batch_size\n",
        "        self.beam_width = beam_width\n",
        "\n",
        "    def process(self,\n",
        "                input_ids: torch.LongTensor,\n",
        "                top_token_scores: torch.FloatTensor,\n",
        "                top_token_indices: torch.LongTensor,\n",
        "                top_token_beam_indices: torch.LongTensor\n",
        "                ):\n",
        "        \"\"\"\n",
        "        :param input_ids: (batch_size * beam_width, current_seq_length), the input_ids that were used to compute top_tokens\n",
        "        :param top_token_scores: (batch_size, 2 * beam_width), representing the score of each top token\n",
        "        :param top_token_indices: (batch_size, 2 * beam_width), representing each token's index (in vocabulary) of the top tokens\n",
        "        :param top_token_beam_indices: (batch_size, 2 * beam_width), representing each token's corresponding beam index of the top tokens\n",
        "\n",
        "        Note: the input arguments `top_token_*` for each sample in batch are sorted from the largest score to the smallest score.\n",
        "        For example, if batch_size = 2 and beam_width = 3, then each of these values denote\n",
        "        top_token_indices[1, 2]: what is the third-best next token for the second sample in the batch?\n",
        "        top_token_scores[0, 1]: what is the score of the second-best next token for the first sample in the batch?\n",
        "        top_token_beam_indices[0, 1]: which beam did we use to generate the second-best next token for the first sample in the batch?\n",
        "\n",
        "        In this function, for each of the top-(2 * beam_width) tokens, we do the following:\n",
        "        (1) If the top token is EOS token:\n",
        "            This means that this hypothesis is done. Therefore, we save the hypothesis so-far to self.finished_beam_hypotheses_list.\n",
        "        (2) If the top token is not EOS token\n",
        "            We have to keep searching with this hypothesis. Therefore, we prepare the hypothesis for next time step.\n",
        "\n",
        "        Returns a dictionary, where\n",
        "        \"unfinished_scores\": size (batch_size * beam_width,), the score of the unfinished beams\n",
        "        \"unfinished_token_indices\": size (batch_size * beam_width,), the index of the last token in the unfinished beams\n",
        "        \"unfinished_beam_indices\": the index of the beam that was used to generate the new unfinished beam\n",
        "        \"\"\"\n",
        "        device = top_token_scores.device\n",
        "\n",
        "        # Initialize unfinished_token_*, which we will return for the next time step.\n",
        "        unfinished_scores = torch.zeros((self.batch_size, self.beam_width), dtype=top_token_scores.dtype).to(device)  # score of the unfinished beams\n",
        "        unfinished_token_indices = torch.zeros((self.batch_size, self.beam_width), dtype=top_token_indices.dtype).to(\n",
        "            device)  # index of the last token of the unfinished beams\n",
        "        unfinished_token_beam_indices = torch.zeros((self.batch_size, self.beam_width), dtype=top_token_beam_indices.dtype).to(\n",
        "            device)  # index of the unfinished beam in the batch\n",
        "\n",
        "        # Loop over the batch\n",
        "        for batch_idx in range(self.batch_size):\n",
        "            # Get sample_beam_hypothesis_list: the finished_beam_hypothesis_list for this sample in the batch\n",
        "            sample_beam_hypothesis_list: BeamHypothesisList = self.finished_beam_hypotheses_list[batch_idx]\n",
        "\n",
        "            # Get the top_token_scores, top_token_indices, top_token_beam_indices for this sample in the batch\n",
        "            # NOTE: size of sample_top_token_*: (2 * beam_width,)\n",
        "            sample_top_token_scores = top_token_scores[batch_idx]\n",
        "            sample_top_token_indices = top_token_indices[batch_idx]\n",
        "            sample_top_token_beam_indices = top_token_beam_indices[batch_idx]\n",
        "\n",
        "            # Loop over all top tokens\n",
        "            sample_beam_idx = 0\n",
        "            for top_token_score, top_token_index, top_token_beam_index in zip(\n",
        "                    sample_top_token_scores, sample_top_token_indices, sample_top_token_beam_indices\n",
        "            ):\n",
        "                # Note that top_token_beam_indices only denotes the index of the beam in each sample.\n",
        "                # We transform this into `beam_idx_in_batch`, where we denote the index of the beam among all (batch_size * beam_width) beams in the batch.\n",
        "                beam_idx_in_batch = batch_idx * self.beam_width + top_token_beam_index\n",
        "\n",
        "                # if top_token == EOS, we add the generation so-far to the beam_hypotheses_list\n",
        "                if top_token_index.item() == eos_token_id:\n",
        "                    # Among the (batch_size * beam_width) input_ids, find the input_ids that correspond to this top_token\n",
        "                    # NOTE: the size of new_input_ids: (seq_len,)\n",
        "                    new_input_ids = input_ids[beam_idx_in_batch]\n",
        "\n",
        "                    # Add the new beam to sample_beam_hypothesis_list\n",
        "                    sample_beam_hypothesis_list.add(\n",
        "                        new_input_ids,\n",
        "                        top_token_score\n",
        "                    )\n",
        "\n",
        "                # if top_token =/= EOS, we aggregate them for next time step.\n",
        "                else:\n",
        "                    # Store the score, token_index, beam_idx_in_batch to the unfinished_scores, unfinished_token_indices, unfinished_token_beam_indices\n",
        "                    unfinished_scores[batch_idx, sample_beam_idx] = top_token_score\n",
        "                    unfinished_token_indices[batch_idx, sample_beam_idx] = top_token_index\n",
        "                    unfinished_token_beam_indices[batch_idx, sample_beam_idx] = beam_idx_in_batch\n",
        "\n",
        "                    sample_beam_idx += 1\n",
        "\n",
        "                # once we have `beam_width` number of new beams, we don't have to add anymore.\n",
        "                if sample_beam_idx == self.beam_width:\n",
        "                    break\n",
        "\n",
        "        # Return the dictionary of unfinished_scores, unfinished_token_indices, unfinished_beam_indices\n",
        "        # Make sure to change the size of each tensor to (batch_size * beam_width,)\n",
        "        return {\n",
        "            \"unfinished_scores\": unfinished_scores.view(-1),  # (batch_size * beam_width,)\n",
        "            \"unfinished_token_indices\": unfinished_token_indices.view(-1),  # (batch_size * beam_width,)\n",
        "            \"unfinished_beam_indices\": unfinished_token_beam_indices.view(-1),  # (batch_size * beam_width,)\n",
        "        }\n",
        "\n",
        "    def finalize(\n",
        "            self,\n",
        "            input_ids: torch.LongTensor,\n",
        "            beam_scores: torch.FloatTensor,\n",
        "    ) -> Tuple[List[torch.LongTensor], List[torch.FloatTensor]]:\n",
        "        \"\"\"\n",
        "        :param input_ids: (batch_idx * beam_width, max_length), input_ids of unfinished beams\n",
        "        :param beam_scores: (batch_idx * beam_width,), scores of unfinished beams\n",
        "        Get the final best beams, among\n",
        "        (1) unfinished beams, for which we get the input_ids and beam_scores as arguments\n",
        "        (2) finished beams, which we store in self.batch_beam_hypothesis_list\n",
        "        Returns a tuple of two lists, where\n",
        "        - tuple[0] is the list of the input_ids of the best beams (length: batch_idx)\n",
        "        - tuple[1] is the list of the scores of the best beams (length: batch_idx\n",
        "        \"\"\"\n",
        "\n",
        "        # 1. Add all unfinished beam hypotheses to self.finished_beam_hypotheses_list\n",
        "        for batch_idx in range(self.batch_size):\n",
        "            # Get sample_beam_hypothesis_list: the finished_beam_hypothesis_list for this sample in the batch\n",
        "            sample_beam_hypothesis_list: BeamHypothesisList = self.finished_beam_hypotheses_list[batch_idx]\n",
        "\n",
        "            for sample_beam_idx in range(self.beam_width):\n",
        "                # Get beam_idx_in_batch: index of the beam in all `batch_size * beam_width` beams in the batch\n",
        "                beam_idx_in_batch = batch_idx * self.beam_width + sample_beam_idx\n",
        "\n",
        "                # Get the input_id for this beam, using `beam_idx_in_batch`\n",
        "                # NOTE: the size of new_input_ids: (seq_len,)\n",
        "                new_input_ids = input_ids[beam_idx_in_batch]\n",
        "\n",
        "                # Get the score of this beam, using `beam_idx_in_batch`\n",
        "                # NOTE: beam_score should be a scalar\n",
        "                beam_score = beam_scores[beam_idx_in_batch].item()\n",
        "\n",
        "                # Add the new hypothesis to sample_beam_hypothesis_list\n",
        "                sample_beam_hypothesis_list.add(new_input_ids, beam_score)\n",
        "\n",
        "        # 2. Select the best hypothesis from each beam_hypothesis_list\n",
        "        best_input_ids = []\n",
        "        best_scores = []\n",
        "        for batch_idx in range(self.batch_size):\n",
        "            # Get sample_beam_hypothesis_list: the finished_beam_hypothesis_list for this sample in the batch\n",
        "            sample_beam_hypothesis_list: BeamHypothesisList = self.finished_beam_hypotheses_list[batch_idx]\n",
        "\n",
        "            # Get best_hypothesis among sample_beam_hypothesis_list (the one with the highest score)\n",
        "            best_hypothesis = max(sample_beam_hypothesis_list.beam_hypotheses, key=lambda hyp: hyp.score)\n",
        "\n",
        "            # Save the input_ids and score of best_hypothesis\n",
        "            best_input_ids.append(best_hypothesis.input_ids)\n",
        "            best_scores.append(best_hypothesis.score)\n",
        "\n",
        "        return best_input_ids, best_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXle5lWRRVma"
      },
      "source": [
        "### Beam Search\n",
        "\n",
        "Conceptually, beam search is a straightforward extension to greedy decoding—where we retain the top-k hypotheses instead of the top-1 hypothesis in greedy decoding. However, implementing it is much more complex than the intuition. While there are many “arbitrary” decisions one needs to make to implement beam search, for the purpose of this assignment, we will do the following:\n",
        "\n",
        "* Compared to greedy decoding, beam search introduces one additional hyper-parameter, beam_width, denoting the number of hypotheses we retain at each time step. For example, set beam_width = 3.\n",
        "\n",
        "* At each time step, we have access to 3 hypotheses $h_1, h_2, h_3$ generated so far, along with the corresponding scores $s_1, s_2, s_3$ (typically defined as the average log probability of tokens in each hypothesis). To update the hypothesis, we compute next token distribution $p_i$ for each hypothesis $h_i$, then take top-`beam_width` tokens (that makes the highest score when appended to the corresponding hypothesis) across all $p_i$s.\n",
        "\n",
        "* At each time step, in some of the hypotheses (say $h_1$), we may encounter the EOS token `<EOS>`. In greedy decoding, we can simply terminate the generation process, but in beam search, there still remain the 2 unfinished hypotheses. In this case, we move the finished hypothesis ($h_1$, which encountered the EOS token) into a separate list. For the remaining unfinished hypotheses $h_2$ and $h_3$, we keep searching for the best next tokens, retrieving the top `beam_width` = 3 tokens to create 3 new hypotheses (and their corresponding scores) for the next time step.\n",
        "\n",
        "* After reaching the max length step, we have two sets of hypotheses—a set of unfinished hypotheses that did not encounter `<EOS>` until the final step, and a set of finished hypotheses. Finally, we compare all hypotheses across the two sets, and return the one with the highest score. Fill in the TODOs in `beam search()`. Given the complexity of the implementation, we provide more\n",
        "structured skeleton for the algorithm, along with two helper classes `BeamHypothesisList` and `BeamManager`. Note that the two classes are already fully implemented, your job is to finish `beam search()` using the\n",
        "two helper classes. You may find the [beam search implementation in Huggingface transformers](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py##L2743) useful. In\n",
        "addition, we leave comments for the expected size of tensor variables as a sanity check.\n",
        "\n",
        "**TODO: fill in the TODOs in `beam_search`.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9JTN6KpYS0P-"
      },
      "outputs": [],
      "source": [
        "# TODO: Complete the implementation of beam search.\n",
        "\n",
        "def beam_search(prompts: List[str], beam_width: int, max_length: int) -> List[str]:\n",
        "    \"\"\"\n",
        "    :param prompts: list of prompt strings\n",
        "    :param beam_width: number of hypotheses in the beam\n",
        "    :param max_length: max generation length\n",
        "    :return: list of generation, including both the original prompt and generation\n",
        "    \"\"\"\n",
        "    # TODO: encode the prompts using tokenizer, to get input_ids and attention_mask\n",
        "    input_encoding = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
        "    input_ids, attention_mask = input_encoding['input_ids'].to(device), input_encoding['attention_mask'].to(device)\n",
        "    \n",
        "    if input_ids.size(-1) > max_length:\n",
        "      raise ValueError(\"Input ID is larger than max_length.\")\n",
        "    \n",
        "    # --- Do not change below --- #\n",
        "    batch_size = input_ids.size(0)\n",
        "    vocab_size = len(tokenizer)\n",
        "    \n",
        "    # initialize model_kwargs\n",
        "    model_kwargs = {\n",
        "        'attention_mask': attention_mask,\n",
        "        'cache_position': torch.arange(input_ids.shape[1], device=device)\n",
        "    }\n",
        "    \n",
        "    # interleave input_ids according to beam_width.\n",
        "    # For example, input_ids for [\"Hi\", \"good\"] with beam_width=3 becomes [\"Hi\", \"Hi\", \"Hi\", \"good\", \"good\", \"good\"]\n",
        "    input_ids, model_kwargs = model._expand_inputs_for_generation(\n",
        "        input_ids=input_ids,\n",
        "        expand_size=beam_width,\n",
        "        is_encoder_decoder=False,\n",
        "        **model_kwargs,\n",
        "    )\n",
        "    # NOTE: the type of `input_ids` and `model_kwargs` are as following:\n",
        "    # input_ids: tensor of size (batch_size * beam_width, seq_len)\n",
        "    # model_kwargs: a dictionary with two elements 'attention_mask', sized (batch_size * beam_width, seq_len), and 'cache_position'\n",
        "    # --- Do not change above --- #\n",
        "    \n",
        "    # TODO: initialize beam_manager\n",
        "    beam_manager = BeamManager(batch_size, beam_width)\n",
        "    \n",
        "    # TODO: initialize unfinished_beam_scores, a tensor of size (batch_size, beam_width) with all elements = 0\n",
        "    unfinished_beam_scores = torch.zeros((batch_size, beam_width), dtype=torch.float).to(device)\n",
        "    \n",
        "    # For each sample in the batch, set all initial beam_score to -1e9, except for the first beam\n",
        "    unfinished_beam_scores[:, 1:] = -1e9\n",
        "    unfinished_beam_scores = unfinished_beam_scores.view(-1)  # (batch_size * beam_width,)\n",
        "    \n",
        "    while True:\n",
        "        # --- Do not change below --- #\n",
        "        model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
        "        # --- Do not change above --- #\n",
        "        \n",
        "        # TODO: run model forward pass with model_inputs as the input\n",
        "        # NOTE: we should set return_dict=True, output_attentions=False and output_hidden_states=False\n",
        "        model_outputs = model(**model_inputs, return_dict=True, output_attentions=False, output_hidden_states=False)\n",
        "        \n",
        "        # TODO compute log_probs for next tokens, using `model_outputs.logits`\n",
        "        # NOTE: size of next_token_scores: (batch_size * beam_width, vocab_size)\n",
        "        next_token_logits = model_outputs.logits[:, -1, :]\n",
        "        next_token_scores = F.log_softmax(next_token_logits, dim=-1)\n",
        "        \n",
        "        # TODO add previous beam_scores to the next_token_scores\n",
        "        # NOTE: size of new_scores: # (batch_size * beam_width, vocab_size)\n",
        "        new_scores = next_token_scores + unfinished_beam_scores.unsqueeze(-1)\n",
        "        \n",
        "        # TODO: retrieve top-(2 * beam_width) next tokens for each sample in the batch\n",
        "        # NOTE: size of `top_token_scores` and `top_token_indices` needs to be: (batch_size, 2 * beam_width)\n",
        "        # NOTE: `top_token_scores` and `top_token_indices` should be sorted from the one with larget score to the one with smallest score (for each sample in batch)\n",
        "        # Hint: use torch.topk with largest=True, sorted=True\n",
        "        # Hint: new_scores needs to be transformed to shape (batch_size, beam_width * vocab_size) prior to topk operation.\n",
        "        top_token_scores, top_token_indices = torch.topk(\n",
        "            new_scores.view(batch_size, beam_width * vocab_size),\n",
        "            k=2 * beam_width,\n",
        "            dim=-1,\n",
        "            largest=True,\n",
        "            sorted=True\n",
        "        )\n",
        "        \n",
        "        # Since top_token_indices are over beam_width * vocab_size, divide it by beam_width to get vocabulary index and beam index\n",
        "        top_token_beam_indices = torch.div(top_token_indices, vocab_size,\n",
        "                                           rounding_mode=\"floor\")  # from which beam the top-token was retrieved from\n",
        "        top_token_indices = top_token_indices % vocab_size  # the index of top-token in the vocabulary\n",
        "        \n",
        "        # TODO: Run beam_manager.process and save the results in unfinished_beam_scores, unfinished_token_indices and unfinished_beam_indices\n",
        "        unfinished_beam_outputs = beam_manager.process(\n",
        "            input_ids,\n",
        "            top_token_scores,\n",
        "            top_token_indices,\n",
        "            top_token_beam_indices\n",
        "        )\n",
        "        unfinished_beam_scores = unfinished_beam_outputs[\"unfinished_scores\"]\n",
        "        unfinished_token_indices = unfinished_beam_outputs[\"unfinished_token_indices\"]\n",
        "        unfinished_beam_indices = unfinished_beam_outputs[\"unfinished_beam_indices\"]\n",
        "        \n",
        "        # --- Prepare input_ids for next time step --- #\n",
        "        # TODO: index input_ids with the unfinished beam indices\n",
        "        # NOTE: input_ids should be (batch_size * beam_width, seq_len)\n",
        "        input_ids = input_ids[unfinished_beam_indices]\n",
        "        \n",
        "        # TODO: concatenate the unfinished_token_indices to the input_ids\n",
        "        input_ids = torch.cat([input_ids, unfinished_token_indices.unsqueeze(-1)], dim=-1)\n",
        "        \n",
        "        # --- Do not change below --- #\n",
        "        # update the model_kwargs according to the concatenated input_ids\n",
        "        model_kwargs = model._update_model_kwargs_for_generation(\n",
        "            model_outputs, model_kwargs, is_encoder_decoder=False\n",
        "        )\n",
        "        # if model_kwargs[\"past_key_values\"] is not None:\n",
        "        #     model_kwargs[\"past_key_values\"] = model._temporary_reorder_cache(\n",
        "        #         model_kwargs[\"past_key_values\"], unfinished_beam_indices,\n",
        "        #     )\n",
        "\n",
        "        if model_kwargs.get(\"past_key_values\") is not None:\n",
        "        # Manually reorder cache for GPT2\n",
        "            reordered_past = ()\n",
        "            for layer_past in model_kwargs[\"past_key_values\"]:\n",
        "                reordered_past += (\n",
        "                    tuple(past_state.index_select(0, unfinished_beam_indices) \n",
        "                        for past_state in layer_past),\n",
        "                )\n",
        "            model_kwargs[\"past_key_values\"] = reordered_past\n",
        "        # --- Do not change above --- #\n",
        "\n",
        "        \n",
        "        # if unfinished input_ids reach the max seq length, exit the loop\n",
        "        if input_ids.size(-1) == max_length:\n",
        "            break\n",
        "    \n",
        "    # TODO: Run beam_manager.finalize to get the best_input_ids and best_scores, among all finished / unfinished beams\n",
        "    best_input_ids, best_scores = beam_manager.finalize(input_ids, unfinished_beam_scores)\n",
        "    \n",
        "    # if len(best_input_ids) < max_length, pad them to the max length\n",
        "    for batch_idx, sample_input_ids in enumerate(best_input_ids):\n",
        "        if sample_input_ids.size(-1) < max_length:\n",
        "            pad_tensor = torch.LongTensor([pad_token_id] * (max_length - sample_input_ids.size(-1))).to(device)\n",
        "            \n",
        "            # TODO: pad best_input_ids with pad_tensor\n",
        "            best_input_ids[batch_idx] = torch.cat([sample_input_ids, pad_tensor])\n",
        "    \n",
        "    # TODO: transform best_input_ids (which is currently a list of tensors) into a tensor of size (batch_idx, max_seq_length)\n",
        "    # Hint: use torch.stack\n",
        "    best_input_ids = torch.stack(best_input_ids)\n",
        "    \n",
        "    return tokenizer.batch_decode(best_input_ids, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.57.1\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Az-Q7QlywzZ"
      },
      "source": [
        "### Sanity check for debugging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fuvQmmYWS3FX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.53.0. You should pass an instance of `Cache` instead, e.g. `past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[\"This restaurant is awesome. I've been here a few\",\n",
              " 'My dog is cute and I love it.\\n\\nRated 5 out of',\n",
              " 'Today is sunny. The sun is shining. The']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sents = [\n",
        "    \"This restaurant is awesome.\",\n",
        "    \"My dog is cute and I love it.\",\n",
        "    \"Today is sunny.\",\n",
        "]\n",
        "\n",
        "beam_search(sents, beam_width=5, max_length=15)\n",
        "\n",
        "# expected output: [\"This restaurant is awesome. I've been here a few\", 'My dog is cute and I love it.\\n\\nRated 5 out of', 'Today is sunny. The sun is shining. The']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYkg7i5ayyy_"
      },
      "source": [
        "### Evaluate Beam Search!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "HGSf-kEUS--e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\rushi\\AppData\\Local\\Temp\\ipykernel_11728\\4107028903.py:51: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
            "Consider using tensor.detach() first. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\autograd\\generated\\python_variable_methods.cpp:837.)\n",
            "  self.worst_score = min(float(hyp.score) for hyp in self.beam_hypotheses)\n",
            "100%|██████████| 20/20 [26:01<00:00, 78.09s/it]\n",
            "100%|██████████| 13/13 [00:15<00:00,  1.16s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beam Search\n",
            "perplexity = 2.05\n",
            "fluency = 0.82\n",
            "diversity = 0.07, 0.13, 0.17\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# If your implementation is efficient enough, the following code will run in no longer than 3 minutes with device = gpu.\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "prompts = [item['prompt'] for item in test_data][:100]\n",
        "MAX_LEN = 100\n",
        "beam_width = 5\n",
        "BATCH_SIZE = 5\n",
        "\n",
        "generations = []\n",
        "for batch_start_idx in tqdm(range(0, len(prompts), BATCH_SIZE)):\n",
        "  batched_prompts = prompts[batch_start_idx: batch_start_idx + BATCH_SIZE]\n",
        "  batched_generations = beam_search(batched_prompts, beam_width, MAX_LEN)\n",
        "\n",
        "  # remove prompt from generation\n",
        "  batched_generations = [generation[len(prompt):] for prompt, generation in zip(batched_prompts, batched_generations)]\n",
        "\n",
        "  generations += batched_generations\n",
        "\n",
        "evaluate(generations, 'Beam Search')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "27zicApAtc7Y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt 0\n",
            "The soccer game was tied 3 to 3 and there was a minute left to play.\n",
            "Generation 0\n",
            "\n",
            "\n",
            "\"I'm not going to lie, I'm not going to lie, I'm not going to lie, I'm not going to lie, I'm not going to lie, I'm not going to lie, I'm not going to lie, I'm not going to lie, I'm not going to lie, I'm not going to lie, I'm not going to lie, I'm not\n",
            "---------\n",
            "\n",
            "Prompt 1\n",
            "Molly loves popcorn.\n",
            "Generation 1\n",
            "\n",
            "\n",
            "\"I love popcorn,\" she said. \"I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn.\n",
            "---------\n",
            "\n",
            "Prompt 2\n",
            "Tim rented a car to visit his ill mother.\n",
            "Generation 2\n",
            "\n",
            "\n",
            "\"It was a very nice day,\" he said.\n",
            "\n",
            "\"It was a very nice day. It was a very nice day. It was a very nice day. It was a very nice day. It was a very nice day. It was a very nice day. It was a very nice day. It was a very nice day. It was a very nice day. It was a very\n",
            "---------\n",
            "\n",
            "Prompt 3\n",
            "Max had been dating Maddie for three Year's.\n",
            "Generation 3\n",
            "\n",
            "\n",
            "Maddie had been dating Maddie for three Year's.\n",
            "\n",
            "Maddie had been dating Maddie for three Year's.\n",
            "\n",
            "Maddie had been dating Maddie for three Year's.\n",
            "\n",
            "Maddie had been dating Maddie for three Year's.\n",
            "\n",
            "Maddie had been dating Maddie for three Year's.\n",
            "\n",
            "Maddie had been dating\n",
            "---------\n",
            "\n",
            "Prompt 4\n",
            "I'm waiting for a payment to come through the mail.\n",
            "Generation 4\n",
            " I'm waiting for a payment to come through the mail. I'm waiting for a payment to come through the mail. I'm waiting for a payment to come through the mail. I'm waiting for a payment to come through the mail. I'm waiting for a payment to come through the mail. I'm waiting for a payment to come through the mail. I'm waiting for a payment to come through the mail\n",
            "---------\n",
            "\n",
            "Prompt 5\n",
            "Kennan was born with a proprioceptive disorder.\n",
            "Generation 5\n",
            "\n",
            "\n",
            "He was diagnosed with Parkinson's disease in the early 1970s.\n",
            "\n",
            "He was diagnosed with Parkinson's disease in the early 1970s.\n",
            "\n",
            "He was diagnosed with Parkinson's disease in the early 1970s.\n",
            "\n",
            "He was diagnosed with Parkinson's disease in the early 1970s.\n",
            "\n",
            "He was diagnosed with Parkinson's disease in the early 1970s.\n",
            "\n",
            "He was diagnosed with Parkinson's\n",
            "---------\n",
            "\n",
            "Prompt 6\n",
            "Rich had worked in the woods for Years.\n",
            "Generation 6\n",
            "\n",
            "\n",
            "\"I'm not going to lie to you,\" he said. \"I'm not going to lie to you. I'm not going to lie to you. I'm not going to lie to you. I'm not going to lie to you. I'm not going to lie to you. I'm not going to lie to you. I'm not going to lie to you. I'm not going\n",
            "---------\n",
            "\n",
            "Prompt 7\n",
            "Tanya wanted to see her sister.\n",
            "Generation 7\n",
            "\n",
            "\n",
            "\"I don't know what you're talking about,\" she said.\n",
            "\n",
            "\"I don't know what you're talking about,\" she said.\n",
            "\n",
            "\"I don't know what you're talking about,\" she said.\n",
            "\n",
            "\"I don't know what you're talking about,\" she said.\n",
            "\n",
            "\"I don't know what you're talking about,\" she said.\n",
            "\n",
            "\"\n",
            "---------\n",
            "\n",
            "Prompt 8\n",
            "One afternoon Sallie took her dog, Gibby, to the dog park.\n",
            "Generation 8\n",
            "\n",
            "\n",
            "\"He was so happy,\" Sallie said. \"He was so happy. He was so happy. He was so happy. He was so happy. He was so happy. He was so happy. He was so happy. He was so happy. He was so happy. He was so happy. He was so happy. He was so happy. He was so happy. He was so happy\n",
            "---------\n",
            "\n",
            "Prompt 9\n",
            "Omar was walking his dog in a new neighborhood.\n",
            "Generation 9\n",
            "\n",
            "\n",
            "\"I was like, 'What the hell is going on?'\" he said. \"I was like, 'What the hell is going on?'\n",
            "\n",
            "\"I was like, 'What the hell is going on?' I was like, 'What the hell is going on?' I was like, 'What the hell is going on?' I was like, 'What the hell is going on?' I\n",
            "---------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# print first 10 generations\n",
        "\n",
        "sampled_prompts = prompts[:10]\n",
        "sampled_generations = generations[:10]\n",
        "\n",
        "for idx, (prompt, generation) in enumerate(zip(sampled_prompts, sampled_generations)):\n",
        "  print(f\"Prompt {idx}\")\n",
        "  print(prompt)\n",
        "  print(f\"Generation {idx}\")\n",
        "  print(generation)\n",
        "  print(\"---------\")\n",
        "  print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
